{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597df42f",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Machine Learning in Geosciences ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3b679",
   "metadata": {},
   "source": [
    "## Gradient Descent - example solution\n",
    "\n",
    "Tasks: \n",
    "\n",
    "* 1/ Import data, define linear model and loss function;\n",
    "\n",
    "* 2/ Define the gradients; \n",
    "\n",
    "* 3/ Define GD update function; \n",
    "\n",
    "* 4/ Set parametrs and run Gradient Descent;  \n",
    "\n",
    "* 5/ Plot the model result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a67931",
   "metadata": {},
   "source": [
    "This Notebook demonstrates how gradient descent finds the solution to a linear regression problem. The illustration is done with pure Python code as well as with plots that show how the solution improves with iterations of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb022f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec91387",
   "metadata": {},
   "source": [
    "### Data \n",
    "\n",
    "Use `data_gd.csv`,  fetature `f2` from the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c770ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust dir path as needed \n",
    "df = pd.read_csv('data_gd.csv', sep=',', header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['f2']\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502d74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data plot \n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046606a",
   "metadata": {},
   "source": [
    "### Task\n",
    "We want to build a regression model that we can use to predict `y` based on other `x`values. Each row in the dataset represents one specific record. \n",
    "Imagine e.g. how glacier velocity flow increases with increase of seasonal temperature. Suppose the relationship is linear, for this case of illustaration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} records to tranin the model.\".format(x.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e84813",
   "metadata": {},
   "source": [
    "### Model: linear regression\n",
    "\n",
    "Reminder: \n",
    "$$ \n",
    "    f(x) = wx + b\n",
    "$$ \n",
    "\n",
    "where `x` is a vector of input features;\n",
    "`y` is a vector of outputs (targets), also response variable; \n",
    "`b` is the bias term (intercept), also often abbreviated as w0\n",
    "`w` is the weight(s) (direction of the linear model) \n",
    "\n",
    "We do not know what the optimal values of w and b are and we want to learn them from data! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bad6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction model\n",
    "def model(x, w, b):\n",
    "    \"\"\"Linear model\n",
    "    \"\"\"\n",
    "    \n",
    "    return w*x + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ad19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model function \n",
    "y_1 = model(x=3, w=0.5, b=0.1)\n",
    "print(y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a122a",
   "metadata": {},
   "source": [
    "### Loss function (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cfaca",
   "metadata": {},
   "source": [
    "**The goal** is find such values of `w` and `b` that minimize the mean squre error (MSE)! \n",
    "\n",
    "$$\n",
    "    Loss = L = \\frac{1}{N} \\sum_{i=1}^{N}(y_i - (wx_i + b))^2 \n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function \n",
    "def loss(x, y, w, b):\n",
    "    \"\"\"Loss function (MSE)  \n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "\n",
    "    # Initialize loss\n",
    "    total_error = 0.0 \n",
    "    for i in range(N):\n",
    "        total_error += (y[i] - (w*x[i] + b))**2\n",
    "\n",
    "    return total_error / N \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b6554",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test the loss function \n",
    "L_1 = loss(x=[3], y=[1.4], w=0.5, b=0.1)\n",
    "print(round(L_1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c8461",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate (here alpha) \n",
    "alpha = 0.001 \n",
    "\n",
    "# Number of epochs to iterate \n",
    "epochs = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdae65",
   "metadata": {},
   "source": [
    "### Model weights initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "w = 0.0\n",
    "b = 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c8e6c",
   "metadata": {},
   "source": [
    "### Select subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da8955",
   "metadata": {},
   "source": [
    "### Calcualte the gradinet\n",
    "\n",
    "To find the partial derivative of the term $$(y_i - (wx + b))^2$$ **with respect to `w`** we apply the chain rule. Here, we have the chain \n",
    "\n",
    "$$f = f_2(f_1)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$f_1 = y_i - (wx + b)$$ and $$f_2 = f_1^2$$\n",
    "\n",
    "To find a partial derivative of f with respect to w we have to first find the partial derivative of f with respect to f2 which is equal to \n",
    "\n",
    "$$2(yi - (wx + b))$$ \n",
    "\n",
    "based on \n",
    "$$ \\frac{\\partial f}{\\partial x}x^2 = 2x$$ \n",
    "\n",
    "---\n",
    "\n",
    "and the partial derivative of of the \n",
    "$$(y_i - (wx + b))$$ \n",
    "**with respect to `b`** is equal of $$-x$$\n",
    "\n",
    "--- \n",
    "\n",
    "This results into two gradients\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}=\\frac{1}{N} \\sum_{i=1}^{N}(2\\cdot(y_i - (wx_i + b))\\cdot(-x_i);\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=\\frac{1}{N} \\sum_{i=1}^{N}(2\\cdot(y_i - (wx_i + b))\\cdot(-1);\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433444b",
   "metadata": {},
   "source": [
    "The gradient for a function is typically denoted with the symbol **$\\nabla$** (nabla). \n",
    "\n",
    "  \n",
    "$$\n",
    "  \\nabla(Loss) = \\left[\\begin{array}{cc}\n",
    "                    \\frac{-2}{N} \\sum_{i=1}^{N}(y_i - (wx_i + b))\\cdot(x_i) \\\\\n",
    "                    \\frac{-2}{N} \\sum_{i=1}^{N}(y_i - (wx_i + b))\n",
    "                 \\end{array}\\right]\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb2b55",
   "metadata": {},
   "source": [
    "If we plug in some vlaues of `w`and `b`, e.g. initial 0 and 0, we get the gradient at the intial point. And with that we have to compute gradient for our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients test calcualtion on sample of data (one point)\n",
    "# i = 0\n",
    "\n",
    "dr_dw = -2 * (y_0 - (w * x_0 + b)) * x_0\n",
    "dr_db = -2 * (y_0 - (w * x_0 + b)) \n",
    "print(dr_dw, dr_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ba9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656066bd",
   "metadata": {},
   "source": [
    "### Update the model \n",
    "\n",
    "We update the model from initial state (or the previous state) by the gradient modulated by the learning rate papmeter. This is an important hyper-parameter of gradient descenttypicaly `Î±` (in Python code typically variable `lr` or `alpha`).  This parameter controls the size of the **update** of the model parameters: \n",
    "\n",
    "$$\n",
    "    w \\leftarrow w - \\alpha\\frac{\\partial L}{\\partial w}, \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "   b \\leftarrow b - \\alpha\\frac{\\partial L}{\\partial b}. \n",
    "$$\n",
    "\n",
    "\n",
    "We substract partial derivatives from the values of parameters because derivatives are indicators of growth of a function, while we want to minimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the update function\n",
    "\n",
    "def update(x, y, w, b, alpha):\n",
    "    \"\"\"Update function, which returns updated parameters. \n",
    "    \"\"\"\n",
    "    dr_dw = 0.0\n",
    "    dr_db = 0.0\n",
    "    N = len(x)\n",
    "\n",
    "    for i in range(N):\n",
    "        dr_dw += -2 * x[i] * (y[i] - (w * x[i] + b))\n",
    "        dr_db += -2 * (y[i] - (w * x[i] + b))\n",
    "\n",
    "    # Update w and b\n",
    "    w = w - (dr_dw/float(N)) * alpha\n",
    "    b = b - (dr_db/float(N)) * alpha\n",
    "\n",
    "    return w, b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169eb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_1, b_1 = update(x[0:1], y[0:1], w, b, alpha) \n",
    "print(f'The initial model weights are updated as follows w = {w_1} and b =  {b_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff573f0",
   "metadata": {},
   "source": [
    "### Gradient descen \n",
    "\n",
    "The Gradien Descent algorithm performs iterative procedure **over the epochs** where we recalculate partial derivatives using the above function, update `w`nand `b`; we **continue the process until convergence**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ab52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient descent iterative function\n",
    "# print the loss and updates on the parameters \n",
    "\n",
    "def gradient_descent(x, y, w, b, alpha, epochs):\n",
    "    \"\"\"Gradient descent process. \n",
    "    \"\"\"\n",
    "\n",
    "    counter = 0;\n",
    "    for e in range(epochs):\n",
    "        w, b = update(x, y, w, b, alpha)\n",
    "\n",
    "        # Log the progress\n",
    "        if (e == 0) or (e < 3000 and e % 200 == 0) or (e % 3000 == 0):\n",
    "            print(\"epoch: \", str(e), \"loss: \"+str(loss(x, y, w, b)))\n",
    "            print(\"w, b: \", w, b)\n",
    "            print('---')\n",
    "            # Plot the update \n",
    "            plt.figure(counter)\n",
    "            axes = plt.gca()\n",
    "            axes.set_xlim([0,50])\n",
    "            axes.set_ylim([0,30])\n",
    "            plt.scatter(x, y)\n",
    "            X_plot = np.linspace(0,50,50)\n",
    "            plt.plot(X_plot, X_plot*w + b, 'r-')\n",
    "            counter += 1\n",
    "    return w, b                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e12700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Gradient Descent algorithm\n",
    "w, b = gradient_descent(x, y, w, b, alpha, epochs)\n",
    "print('End of training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa2593",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The final coeficients: {}, {}'.format(w, b)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af3e7b",
   "metadata": {},
   "source": [
    "### Model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = 25.0\n",
    "y_new = model(x_new, w, b)\n",
    "print(y_new) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
